# example.conf: A single-node Flume configuration

# Name the components on this agent
event.sources = r1
event.sinks = k1
event.channels = c1

# Describe/configure the source
# kafka source

#event.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
#event.sources.r1.channels = c1
#event.sources.r1.batchSize = 5000
#event.sources.r1.batchDurationMillis = 2000
#event.sources.r1.kafka.bootstrap.servers = 192.168.1.230:9092
#event.sources.r1.kafka.zookeeperConnect = 192.168.1.230:2181
#event.sources.r1.kafka.topics = mytopic
#event.sources.r1.kafka.consumer.group.id = aha

event.sources.r1.type = com.appleframework.ums.server.storage.hdfs.KafkaSource
event.sources.r1.channels = c1
event.sources.r1.zookeeperConnect = apple.master:2181,apple.slave1:2181,apple.slave2:2181
event.sources.r1.topic = topic_clientdata
event.sources.r1.groupId = flume_ng
#event.sources.r1.kafka.consumer.timeout.ms = 100
event.sources.r1.kafka.auto.offset.reset = smallest
#event.sources.r1.kafka.auto.commit.interval.ms = 1000
#event.sources.r1.kafka.zookeeper.sync.time.ms = 200
#event.sources.r1.kafka.zookeeper.session.timeout.ms = 400
event.sources.r1.batchSize=10000

# Describe the sink
#event.sinks.k1.type = logger

# hdfs sink
event.sinks.k1.type = com.appleframework.flume.sink.hdfs.HDFSEventSink
event.sinks.k1.channel = c1
#event.sinks.k1.hdfs.path = /flume/event/%Y-%m-%d
#event.sinks.k1.hdfs.path = hdfs://apple.master:9000/flume2/event/%Y-%m-%d
#event.sinks.k1.hdfs.filePrefix = event171
event.sinks.k1.hdfs.fileType = DataStream
event.sinks.k1.hdfs.writeFormat = Text
event.sinks.k1.hdfs.round = true
event.sinks.k1.hdfs.roundValue = 60
event.sinks.k1.hdfs.roundUnit = minute
event.sinks.k1.hdfs.rollInterval = 3600
event.sinks.k1.hdfs.useLocalTimeStamp = true
event.sinks.k1.hdfs.rollSize = 102400000
event.sinks.k1.hdfs.rollCount = 0
event.sinks.k1.hdfs.batchSize = 10000

#event.sinks.k1.dfs.client.failover.proxy.provider.cluster1=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
#event.sinks.k1.dfs.nameservices=cluster1
#event.sinks.k1.dfs.namenode.rpc-address.cluster1.nn1=hdpc1-dn001:8020
#event.sinks.k1.dfs.namenode.rpc-address.cluster1.nn2=hdpc1-dn001:8020
#event.sinks.k1.dfs.ha.namenodes.cluster1=nn1,nn2
#event.sinks.k1.fs.defaultFS=hdfs://cluster1

#event.sinks.k1.hdfs.path= /flume2/event/%Y-%m-%d

# Use a channel which buffers events in memory
#event.channels.c1.type = memory
event.channels.c1.capacity = 100000
event.channels.c1.transactionCapacity = 10000

# use file channel
event.channels.c1.type = file
event.channels.c1.checkpointDir = /data/flume/checkpoint/error
event.channels.c1.dataDirs = /data/flume/data/error

# Bind the source and sink to the channel
event.sources.r1.channels = c1
event.sinks.k1.channel = c1
