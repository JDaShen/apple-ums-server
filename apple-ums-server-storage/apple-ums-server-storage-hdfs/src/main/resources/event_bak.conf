# example.conf: A single-node Flume configuration

# Name the components on this agent
event.sources = r1
event.sinks = k1
event.channels = c1

# Describe/configure the source
# kafka source

event.sources.r1.type = com.appleframework.ums.server.storage.hdfs.KafkaSource
event.sources.r1.channels = c1
#event.sources.r1.zookeeperConnect = apple.master:2181,apple.slave1:2181,apple.slave2:2181
#event.sources.r1.topic = topic_clientdata
#event.sources.r1.groupId = flume_ng
event.sources.r1.kafka.consumer.timeout.ms = 100
event.sources.r1.kafka.auto.offset.reset = smallest
event.sources.r1.kafka.auto.commit.interval.ms = 1000
event.sources.r1.kafka.zookeeper.sync.time.ms = 200
event.sources.r1.kafka.zookeeper.session.timeout.ms = 400
event.sources.r1.batchSize=10000

# Describe the sink
#event.sinks.k1.type = logger

# hdfs sink
event.sinks.k1.type = com.appleframework.flume.sink.hdfs.HDFSEventSink
event.sinks.k1.channel = c1
#event.sinks.k1.hdfs.path = /flume/event/%Y-%m-%d
#event.sinks.k1.hdfs.path = hdfs://apple.master:9000/flume2/event/%Y-%m-%d
#event.sinks.k1.hdfs.filePrefix = event171
event.sinks.k1.hdfs.fileType = DataStream
event.sinks.k1.hdfs.writeFormat = Text
event.sinks.k1.hdfs.round = true
event.sinks.k1.hdfs.roundValue = 60
event.sinks.k1.hdfs.roundUnit = minute
event.sinks.k1.hdfs.rollInterval = 3600
event.sinks.k1.hdfs.useLocalTimeStamp = true
event.sinks.k1.hdfs.rollSize = 102400000
event.sinks.k1.hdfs.rollCount = 0
event.sinks.k1.hdfs.batchSize = 10000

# use file channel
event.channels.c1.type = file
event.channels.c1.capacity = 100000
event.channels.c1.transactionCapacity = 10000
event.channels.c1.checkpointDir = /work/data/flume/checkpoint/error
event.channels.c1.dataDirs = /work/data/flume/data/error

# Bind the source and sink to the channel
event.sources.r1.channels = c1
event.sinks.k1.channel = c1