# example.conf: A single-node Flume configuration

# Name the components on this agent
event.sources = r1
event.sinks = k1
event.channels = c1

# Describe/configure the source
# kafka source

event.sources.r1.type = com.appleframework.ums.server.storage.hdfs.KafkaSource
event.sources.r1.channels = c1
event.sources.r1.kafka.consumer.timeout.ms = 100
event.sources.r1.kafka.auto.offset.reset = smallest
event.sources.r1.kafka.auto.commit.interval.ms = 1000
event.sources.r1.kafka.zookeeper.sync.time.ms = 200
event.sources.r1.kafka.zookeeper.session.timeout.ms = 400
event.sources.r1.batchSize=10000

# Describe the sink
#event.sinks.k1.type = logger

# hdfs sink
event.sinks.k1.type = com.appleframework.flume.sink.hdfs.HDFSEventSink
event.sinks.k1.channel = c1
event.sinks.k1.hdfs.fileType = DataStream
event.sinks.k1.hdfs.writeFormat = Text
event.sinks.k1.hdfs.round = true
event.sinks.k1.hdfs.roundValue = 60
event.sinks.k1.hdfs.roundUnit = minute
event.sinks.k1.hdfs.rollInterval = 3600
event.sinks.k1.hdfs.useLocalTimeStamp = true
event.sinks.k1.hdfs.rollSize = 102400000
event.sinks.k1.hdfs.rollCount = 0
event.sinks.k1.hdfs.batchSize = 10000

# use file channel
event.channels.c1.type = com.appleframework.flume.file.channel.FileChannel
event.channels.c1.capacity = 100000
event.channels.c1.transactionCapacity = 10000

# Bind the source and sink to the channel
event.sources.r1.channels = c1
event.sinks.k1.channel = c1
